/*
"Copyright 2019 Intel Corporation.

The source code, information and material ("Material") contained herein is owned by Intel Corporation or its
suppliers or licensors, and title to such Material remains with Intel Corporation or its suppliers or licensors.
The Material contains proprietary information of Intel or its suppliers and licensors. The Material is
protected by worldwide copyright laws and treaty provisions. No part of the Material may be used, copied,
reproduced, modified, published, uploaded, posted, transmitted, distributed or disclosed in any way without Intel's
prior express written permission. No license under any patent, copyright or other intellectual property rights in
the Material is granted to or conferred upon you, either expressly, by implication, inducement, estoppel or otherwise.
Any license under such intellectual property rights must be express and approved by Intel in writing.


Include supplier trademarks or logos as supplier requires Intel to use, preceded by an asterisk. An asterisked footnote
can be added as follows: *Third Party trademarks are the property of their respective owners.

Unless otherwise agreed by Intel in writing, you may not remove or alter this notice or any other notice
embedded in Materials by Intel or Intel's suppliers or licensors in any way."
*/



// OpenVinoTestDll.cpp : Defines the entry point for the application.
//

#include "OpenVinoData.h"

#include <iomanip>
#include <fstream>
#include <vector>
#include <memory>
#include <cstdlib>
#include <string>
#include <limits>

#include <opencv2/opencv.hpp>
#include <ie/inference_engine.hpp>

using namespace std;
using namespace InferenceEngine;

/*
 * @brief Initialize OpenVino with passed model files
 * @param modelXmlFilePath
 * @param modelBinFilePath
 * @param modelLabelFilePath
 */
void 
OpenVinoData::Initialize(
	string modelXmlFilePath,
	string modelBinFilePath,
	int inferWidth,
	int inferHeight)
{

	// --------------------------- 1. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
	clog << "2. Read IR..." << endl;
	Core core;
	/** Set batch size to 1 **/
	CNNNetwork network = core.ReadNetwork(modelXmlFilePath);
	auto shapes = network.getInputShapes();
	for (auto& shape : shapes)
		shape.second[0] = 1;
	network.reshape(shapes);


	// --------------------------- 2. Configure input & output ---------------------------------------------
	clog << "3. Configure input/output..." << endl;
	input_info = network.getInputsInfo().begin()->second;
	input_name = network.getInputsInfo().begin()->first;


	cv::Size& new_input_resolution = cv::Size(inferWidth, inferHeight);
	SizeVector input_dims = input_info->getTensorDesc().getDims();
	input_dims[0] = 1;
	if (new_input_resolution != cv::Size()) {
		input_dims[2] = static_cast<size_t>(new_input_resolution.height);
		input_dims[3] = static_cast<size_t>(new_input_resolution.width);
	}

	std::map<std::string, SizeVector> input_shapes;
	input_shapes[network.getInputsInfo().begin()->first] = input_dims;
	network.reshape(input_shapes);

	input_info->setLayout(Layout::NCHW);
	input_info->setPrecision(Precision::U8);

	DataPtr output_info = network.getOutputsInfo().begin()->second;
	output_name = network.getOutputsInfo().begin()->first;

	output_info->setPrecision(Precision::FP32);

	// --------------------------- 3. Loading model to the plugin ------------------------------------------
	clog << "4. Loading model..." << endl;
	executable_network = core.LoadNetwork(network, "CPU");//, cnnConfig.execNetworkConfig);

	clog << "Intialized." << endl;
}

/*
 * @brief Call infer using loaded model files
 * @param filePath
 * @param modelBinFilePath
 * @param modelLabelFilePath
 */
bool
OpenVinoData::Infer(
	std::string filePath, int* w, int* h, float* out)
{
	// --------------------------- 5. Create infer request -------------------------------------------------
	clog << "5. Creating request..." << endl;
	InferRequest infer_request = executable_network.CreateInferRequest();

	// return image_file_name;

	// --------------------------- 6. Prepare input --------------------------------------------------------
	cout << "6. Prepare input..." << endl;
	int height = 1080;
	int width = 1920;
	//cv::Mat image(height, width, CV_8UC4, texture);
	cv::Mat image = cv::imread(filePath);
	cv::Mat outputImage;
	cv::cvtColor(image, image, cv::COLOR_BGRA2RGB);

	/* Resize manually and copy data from the image to the input blob */
	Blob::Ptr input = infer_request.GetBlob(input_name);
	auto input_data = input->buffer().as<PrecisionTrait<Precision::U8>::value_type*>();

	auto size = cv::Size(input_info->getTensorDesc().getDims()[3], input_info->getTensorDesc().getDims()[2]);
	cv::resize(image, image, size);

	size_t channels_number = input->getTensorDesc().getDims()[1];
	size_t image_size = input->getTensorDesc().getDims()[3] * input->getTensorDesc().getDims()[2];

	for (size_t pid = 0; pid < image_size; ++pid) {
		for (size_t ch = 0; ch < channels_number; ++ch) {
			input_data[ch * image_size + pid] = image.at<cv::Vec3b>(pid)[ch];
		}
	}
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 7. Do inference --------------------------------------------------------
	clog << "7. Do inference..." << endl;
	/* Running the request synchronously */
	infer_request.Infer();
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 8. Process output ------------------------------------------------------
	clog << "8. Process output..." << endl;
	Blob::Ptr output = infer_request.GetBlob(output_name);
	auto output_shape = output->getTensorDesc().getDims();
	int length = output_shape[0] * output_shape[1] * output_shape[2] * output_shape[3];
	LockedMemory<const void> blobMapped = as<MemoryBlob>(output)->rmap();
	float* output_data_pointer = blobMapped.as<float*>();
	std::vector<float> output_data(output_data_pointer, output_data_pointer + length);

	//align result dimension
	//output_data  = transpose4d(output_data, ieSizeToVector(output_shape), { 0, 3, 1, 2 });
	int rows = output_shape[2];
	int cols = output_shape[3];
	if (output_data.size() == rows * cols * 3) // check that the rows and cols match the size of your vector
	{
		//copy vector to mat
		cv::Mat channelR(rows, cols, CV_32FC1, output_data.data());
		cv::Mat channelG(rows, cols, CV_32FC1, output_data.data() + cols * rows);
		cv::Mat channelB(rows, cols, CV_32FC1, output_data.data() + 2 * cols * rows);
		// RGB2BGR
		std::vector<cv::Mat> channels{ channelB, channelG, channelR };

		// Create the output matrix
		merge(channels, outputImage);
	}
	//postprocessing
	// normolize  (-1,1) to (0,255)
	cv::normalize(outputImage, outputImage, 0, 255, cv::NORM_MINMAX);

	int arraysize = outputImage.rows * outputImage.cols * outputImage.channels();
	memcpy(out, outputImage.data, arraysize * sizeof(float));
	*w = outputImage.size().width;
	*h = outputImage.size().height;

	//cv::imwrite("test1.png", outputImage);
	return true;
}

/*
 * @brief Call infer using loaded model files
 * @param filePath
 * @param modelBinFilePath
 * @param modelLabelFilePath
 */
bool
OpenVinoData::Infer(
	unsigned char* inferdata, int inwidth, int inheight, unsigned char* out, bool debug_flag)
{
	// --------------------------- 5. Create infer request -------------------------------------------------
	clog << "5. Creating request..." << endl;
	InferRequest infer_request = executable_network.CreateInferRequest();

	// return image_file_name;

	// --------------------------- 6. Prepare input --------------------------------------------------------
	cout << "6. Prepare input..." << endl;

	cv::Mat outputImage;
	cv::Mat image(inheight, inwidth, CV_8UC3, inferdata);
	if (debug_flag)
	{
		cv::imwrite("input.png", image);
	}
	/*
	cv::Mat image = cv::imread(filePath);
	cv::cvtColor(image, image, cv::COLOR_BGRA2RGB);*/
	
	/* Resize manually and copy data from the image to the input blob */
	Blob::Ptr input = infer_request.GetBlob(input_name);
	auto input_data = input->buffer().as<PrecisionTrait<Precision::U8>::value_type*>();

	
	auto size = cv::Size(input_info->getTensorDesc().getDims()[3], input_info->getTensorDesc().getDims()[2]);
	cv::resize(image, image, size);

	size_t channels_number = input->getTensorDesc().getDims()[1];
	size_t image_size = input->getTensorDesc().getDims()[3] * input->getTensorDesc().getDims()[2];

	for (size_t pid = 0; pid < image_size; ++pid) {
		for (size_t ch = 0; ch < channels_number; ++ch) {
			input_data[ch * image_size + pid] = image.at<cv::Vec3b>(pid)[ch];
		}
	}
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 7. Do inference --------------------------------------------------------
	clog << "7. Do inference..." << endl;
	/* Running the request synchronously */
	infer_request.Infer();
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 8. Process output ------------------------------------------------------
	clog << "8. Process output..." << endl;
	Blob::Ptr output = infer_request.GetBlob(output_name);
	auto output_shape = output->getTensorDesc().getDims();
	int length = output_shape[0] * output_shape[1] * output_shape[2] * output_shape[3];
	LockedMemory<const void> blobMapped = as<MemoryBlob>(output)->rmap();
	float* output_data_pointer = blobMapped.as<float*>();
	std::vector<float> output_data(output_data_pointer, output_data_pointer + length);

	//align result dimension
	//output_data  = transpose4d(output_data, ieSizeToVector(output_shape), { 0, 3, 1, 2 });
	int rows = output_shape[2];
	int cols = output_shape[3];
	if (output_data.size() == rows * cols * 3) // check that the rows and cols match the size of your vector
	{
		//copy vector to mat
		cv::Mat channelR(rows, cols, CV_32FC1, output_data.data());
		cv::Mat channelG(rows, cols, CV_32FC1, output_data.data() + cols * rows);
		cv::Mat channelB(rows, cols, CV_32FC1, output_data.data() + 2 * cols * rows);
		// RGB2BGR
		std::vector<cv::Mat> channels{ channelB, channelG, channelR };

		// Create the output matrix
		merge(channels, outputImage);
	}
	//postprocessing
	// normolize  (-1,1) to (0,255)
	cv::normalize(outputImage, outputImage, 0, 255, cv::NORM_MINMAX);

	outputImage.convertTo(outputImage, CV_8U);
	if(debug_flag)
	{
		cv::imwrite("output.png", outputImage);
	}
	
	int arraysize = outputImage.rows * outputImage.cols * outputImage.channels();
	memcpy(out, outputImage.data, arraysize * sizeof(unsigned char));

	//cv::imwrite("test1.png", outputImage);
	return true;
}
